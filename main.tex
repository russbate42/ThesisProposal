\documentclass[12pt]{article}

\usepackage{import} % for importing alternative to include\input
\input{preamble}
\input{geometry}
\input{commands}
\graphicspath{{figures/}}
% \usepackage{glossaries}
% \input{header}

\usepackage{silence}
\WarningsOff[lipsum]
\WarningFilter[lipsum]{lipsum}{Package lipsum Warning}
\usepackage{lipsum}
\import{./}{dummy-text-generator.tex}

% Math command
\newcommand{\pd}{\partial}

% For glossary without glossaries package
\newcommand{\term}[2]{\textbf{#1}: #2}
\newcommand{\acronym}[2]{\textbf{#1} -- #2}

\begin{document}
\ActivateWarningFilters[lipsum]
\import{./}{title-page.tex}

\newpage
\section{Introduction}
This article is a thesis proposal for a Doctorate of Philosophy from the Physics
and Astronomy Department at the University of British Columbia. The research
presented here uses data from the Large Hadron Collider (LHC) at the European
Organization for Nuclear Research (CERN). This proposal covers key topics
related to my thesis. Firstly, a terse outline of the ATLAS detector will be
given as it pertains to the recorded data used in the research discussed here.
Secondly I will outline my work with the organization and other research
projects I have been involved with thus far. Next it will cover the topics
pertaining to my thesis, namely theoretical motivation, a brief outline of
previous research on this topic, as well as new techniques and methods being
applied. We will look at progress thus far in the new studies as well
as preliminary results. Finally, an outline is made of the proposed timeline for
completion.

\section{The ATLAS Experiment}
The ATLAS experiment is a general purpose particle detector. It stands for
\textbf{A} \textbf{T}oroidal \textbf{L}HC \textbf{A}pparatu\textbf{S} getting
it's name from the toroidal magnet system which is integral to the design which
plays an important role in physics observations. ATLAS is designed to observe
collisions from the full LHC beam energy and intensity. The overall dimensions
are 44 meters across, with a diameter of 25 meters, and weighs over 7000 tonnes
\cite{Aad_2024}. \begin{wrapfigure}{l}{0.42\textwidth}
    \centering
    \includegraphics[width=0.4\textwidth]{Aerial_view_of_the_area_where_the_LHC_ring_is_located}
    \caption{Aerial view of the LHC, located in Geneva, Switzerland.}
    \label{fig:aerial_view}
\end{wrapfigure} As a general purpose particle detector it is a composite of
many different subdetector systems, each with their own physics objectives in
mind. The detector covers nearly the entire solid angle around the
collision point \cite{The_ATLAS_Collaboration_2008}. 

The LHC beam can supply protons and heavy ion nuclei at energies up to $13.6$
TeV \cite{Aad_2024}. Particles are injected into the LHC ring in bunches and are
held in place with Radio Frequency (RF) electric fields
\cite{bunch_filling_schemes_bailey}. When injections are complete, the beam
energy is slowly ramped up to target operational energies. Bunches are guided
around the LHC ring with dipole magnets \cite{The_ATLAS_Collaboration_2008,
bunch_filling_schemes_bailey}. At these energies, particles are highly Lorentz
boosted, and have a bunch crossing spacing of $25$ ns. Collisions occur at the
crossing point located at the center of the detector. Beam optics govern the
number of collisions per bunch crossing through the density of bunches and
accuracy of the collision point. Beam collimation, and collision geometries are
grouped into a single parameter at the LHC - $\beta^*$ \cite{Aad_2024}. The many
collisions that occur during a single bunch crossing is known as \textit{in time
pile-up} and is paramaterized by $\langle\mu\rangle$. This depends on beam
conditions during each data taking year. The data used for the analysis proposed
in this paper is shown in Figure \ref{fig:lhc_luminosity}, as measured in units
of integrated luminosity.

\begin{figure}[t!]
    \centering
    \begin{subfigure}[t]{.48\textwidth}
        \centering
        \includegraphics[width=\columnwidth]{intlumivstimeRun2}
        \caption{Luminosity collected from the full LHC Run 2.}
        \label{subfig:lumi_run2}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{.48\textwidth}
        \centering
        \includegraphics[width=\columnwidth]{intlumivstimeRun3}
        \caption{Luminosity collected from the partial LHC Run 3.}
        \label{subfig:lumi_run3}
    \end{subfigure}
\caption{Luminosity delivered to ATLAS from the LHC for the duration of data
taking for the analysis - that is, Full Run 2 data at $\sqrt{s}=13\un{TeV}$, and
partial Run 3 at $\sqrt{s}=13.6\un{TeV}$.} %(cite me)
\label{fig:lhc_luminosity}
\end{figure}

\section{Research Contributions to ATLAS}

% \section{Machine Learning for Pions}
% \begin{figure}[h]
%     \centering
%     \begin{subfigure}[t]{.48\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{EnergyIQROverMedian}
%         \caption{Neural network response demonstrating energy reconstruction
%         median and the interquartile range.}
%         \label{sufig:ml4p_energy_response}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[t]{.48\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{clusters_tracks_combined}
%         \caption{Figure demonstrating the energy response complementa}
%         \label{subfig:complimentarity}
%     \end{subfigure}
% \caption{Main results from the machine learning for pions project.}
% \label{fig:ml4p_main_results.}
% \end{figure}


\subsection{Machine Learning for Pions}
My first research project within ATLAS as with the Machine Learning for Pions
project. At ATLAS, an inportant step in physics analysis is the reconstruction
of incident particle energy. One decay mechanism for a particle with some high
initial energy is to form energetic particle showers within the detector. Thus
with the primary goal in mind of reconstructing fundamental interactions, we
seek to accurately reconstruct the total energy of particle showers, henceforth
referred to as \textit{jets}. The Machine Learning for Pions project had a goal
of investigating if machine learning techniques could improve not only energy
reconstruction but also incident particle classification as well
\cite{ml4p_prelim}. This was done with Pions due the fact that as a light
hadron, they are produced in abundance at the LHC and are a primary
reconstruction object within a jet. This research project was largely a success,
demonstrating that deep learning was able to better reconstruct energies of jet
substructure elements when compared with the standard technique used within
ATLAS of Local Cell Weighting (LCW) \cite{ml4p}. Moreoever, we were able to
demonstrate that the use of complimentary information from the detector
subsystems performed better than using this information on an individual basis
\cite{ml4p}.
% See Appendix \ref{subfig:complimentarity} for the relevant plot, demonstrating
% subdetector complementarity.
% Add citation for LCW

\subsection{Time at CERN}
During my PhD I was given the opportunity to travel to the LHC and work on site
at CERN. This was a valuable opportunity to collaborate in person with
researchers from my team on various projects. In addition, I was able to take
part in underground visits to see the ATLAS detector in person. Beyond seeing
the detector in person, I was able to get trained and take shifts in the ATLAS
control room. My responsibilities involved two positions, the Run Controller,
and the Trigger Shifter. The Run controller is responsible for starting and
stopping data daking runs, and choosing which subcomponents of the detector are
involved in recording data. The trigger shifter is responsible for monitoring
data rates, CPU processing usage, and controlling which events we will record,
as dictated by the \textit{trigger menu} \cite{trigger_2017, HLT_2016}. As a
result, I actively took part in data taking for some of the data used in this
analysis, as shown in Figure \ref{fig:lhc_luminosity}.



\subsection{Authorship Qualification Project}
Active collaborators with the ATLAS experiment must complete some task that is
needed by the experimental apparatus (or software) in order to qualify for
authorship on papers. For this task, I worked on studies related to an important
step in data processing called \textit{jet cleaning}. When we take data with
ATLAS, not all of the jets which we record are from the physics events we are
interested in. Jets which are reconstructed which are \textit{not} from
proton-proton collisions are referred to as \textit{fake jets}. There are three
main sources of fake jets. These are calorimetry noise, beam induced background,
or events from cosmic muons causing showers in the detector. In order to remove
these events, we apply jet cleaning cuts to the data. The strength of cuts are
fixed at given \textit{working points}. It was my task to investigate if these
cuts were working as expected for the newest internal software releases
\cite{atlas_simulation}, and characterize their performance. In addition,
studies were performed on how we might possibly optimize some of these cuts.
These studies are performed with small radius Particle Flow jets
\cite{pflow_jets}.

Within ATLAS, we do not simulate fake jet events, and therefore we have no truth
information to characterize the physics. This means that the use of supervised
learning is not possible. Therefore, cuts are developed based on intuitive
physics. For example, examining the energy distribution of an event. If energy
is deposited horizontally within the detector, there is a higher likelihood that
it came from beam induced background which are approximately parallel with the
beam axis. Early cleaning cuts were developed based on known hardware issues
such as calorimetry noise. Due to the types of selections that are being made
and the instrinsic difficulty of the problem, there is considerable room for
optimization. 

\begin{figure}[t!]
    \centering
    \begin{subfigure}[t]{.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{GoodJets_FracSamplingMax_EMFrac}
        \caption{Balanced di-jet sample showing physics of well behaved jets.}
        \label{subfig:good_jets_th2}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{FakeJets_FracSamplingMax_EMFrac}
        \caption{Fake jet enriched sample demonstrating a strong bias towards
        distribution tails.}
        \label{subfig:fake_jets_th2}
    \end{subfigure} %
    ~
    \begin{subfigure}[t]{.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Trees_NewDef_R22_Eta_Efficiency_Tight}
        \caption{Jet cleaning efficiency for the tight working point,
        characterized accross $|\eta|$.}
        \label{subfig:cleaning_perf_th2_loose}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Trees_NewDef_R22_FakeRejection_Tight}
        \caption{Fake jet rejection for the tight working point as a function of
        $p_T$.}
        \label{subfig:cleaning_perf_th2_tight}
    \end{subfigure}
\caption{Jet cleaning studies, showing kinematic distributions for balanced
di-jets and fakd jet enriched samples, as well as the efficiency and rejection
rates for the tight working point.}
\label{fig:jet_cleaning}
\end{figure}
Optimization studies were performed by searching for ``bumps'' in
distribution tails. In order to aid these studies, two important subsamples of
collision data were created. Firstly, we look at kinematically balanced di-jet
samples. There is no guarantee that all events in this sample are from $pp$
scattering events, however due to the nature of the selections we can assume a
high purity. The second category is one which has a higher likelihood of
contamination from fake jet events. For this, selections are used such as jets
being \textit{out of time}, or events which have high missing transverse
momentum ($p_T$). These two samples are compared in both one and two dimensions with
variables that we believe to be of interest for the cleaning cuts. It is clear
that the enriched fake jet sample has properties in the tails of the
distributions that are indicative of sample contamination. See Figure
\ref{subfig:fake_jets_th2}. 

The performance of these cuts can be characterized with respect to two metrics.
First, we measure the cut efficiency using the balanced di-jet sample. This is
done with the \textit{tag and probe} method. Simply put, tag and probe relies on
the assumption that in our balanced di-jet sample, if the leading $p_T$ jet is a
fake, so too is it's counterpart. Applying cleaning cuts to the balanced di-jet
sample on \textit{both} leading \textit{and} subleading jets, denominated by
cuts applied to \textit{only} leading \textit{or} subleading, yields an
approximation for efficiency. Secondly we measure the number of jets rejected
from the fakes, keeping in mind that the degree of purity is unkown, and
therefore rejecting more fakes is not necessarily better.  

Three working points for these cuts are chosen - loose, tight, and tenacious -
each with successively increasing fake jet rejection, and decreasing efficiency.
The performance of these cuts has been characterized across detector geometries
and transverse momentum distributions for each working point. This is performed
for both Run 2 and Run 3 data. These working points were integrated into the
newest ATLAS Athena software \cite{atlas_simulation} releases.

\section{Resonant Analysis of Boosted Di-Higgs to Four Bottom Quarks
$hh\rightarrow b\overline{b}b\overline{b}$}
\subsection{Theoretical Motivation}

The Higgs boson is an interesting particle to perform direct searches with
because there is such a rich ecosystem of physics beyond the standard model that
can \textit{and should} in principle couple to the higgs. In general, unless
there is a specific search that is extremely well motivated, it is a worthwhile
endeavour to do the broadest search possible in order to capture as much of this
ecosystem at once. Therefore, it is in the experimentalists best interest to
keep any direct search as model agnostic as possible. With that in mind, there
is such an idea that has not yet been ruled out experimentally and is very well
motivated by one of the largest questions in theoretical physics. That is,
\textit{big hierarchy problem}. Namely, \textit{why is gravity so weak in
comparison with other forces?}. The theoretical construct in question is that of
Extra Dimensions. This idea is not new, and in fact has been around for a long
time, since the 1920s \cite{KALUZA_2018}. A phenomelogical consequence of extra
dimensions is a spin-2 graviton which under no other assumptions can couple to
the Higgs boson \cite{bsm}.  

The obvious extension is that of flat extra dimensions. It is easy to extend
flat Minkowski space by describing the metric as $\eta = (-1,1,1,1,1)$. However,
one of the consequences of having large flat extra dimensions is that we may
expect some deviations from the inverse square law of gravity \cite{bsm}. Some
experiments have been devised to measure this deviation at small scale, and a
large amount of phase space has been ruled out \cite{bsm}. However, this is
dependent upon the number of extra dimensions. If we add extra dimensions of
$n\geq 2$, it is still possible to provide some natural explanation for the
hierarchy problem. Gravity is weak in comparison to Electro-Weak (EW) forces,
because EW is confined to the three spatial dimensions, and the graviton is able
to propagate in some higher dimensional manifold. These are known as as Large
Extra Dimension (LED), or Arkani-Hamed, Dimopoulos, Dvali (ADD) models
\cite{Arkani_Hamed_1998, Arkani_Hamed_1999}. 

It is not difficult to imagine a slightly more complex geometry where instead of
additional flat dimensions we can add some curvature. One way to achieve this is
to simply form a closed loop with one of the dimensions. This process is
generally referred to as \textit{compactification}. In the most simple case of a
compactified dimension, the closed loop creates a closed boundary condition. As
a result, resonance can occur with higher order excitations of fields on this
manifold. 

\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{RSG_resonant_modes.png}
        \caption{Visual representation of the resonant modes resulting from the
        solution of the equation in warped extra dimensions. The resonant modes
        are pushed out to the boundary which is the energy limit for the physics we
        wish to study \cite{bsm}.}
        \label{subfig:RSG_resonant_modes}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{warp_factor}
        \caption{Graphic representation of the relative strength of gravity
        spreading out at the boundary \cite{bsm}.}
        \label{subfig:warp_factor}
    \end{subfigure}
\caption{}
\label{fig:theoretical_motivation_figure}
\end{figure}

% Maybe swap these two paragraphs
First let us make some fundamental observations about allowing for extra
dimensions. Momentum in a five (or higher) dimensional spacetime can show up as
extra mass in four-space \cite{KALUZA_2018, bsm}. 
\begin{align}
    E = \sqrt{\vb{p}^2 + p_5^5 + m^2} \\
    E = \sqrt{\vb{p}^2 + m_{KK}}
\end{align}
Crucially, this is only the case for particles which have access to the 5-D
bulk. Standard model particles are restricted to three spatial dimensions.
Solving for the 5-D action potenial of a scalar particle on the simple
compactified 5-D manifold configuration, and allowing the wavefunction to be
periodic, allows for distinct resonant modes to form. Namely, we have
\begin{equation}
    m^2 = m_0^2 + \frac{n_5^2}{R_5^2}
\end{equation}
Where $R$ is the radius of the compactified dimension, and we have the $n^{th}$
order field excitation. These are what are known as Kaluza-Klein mass towers
\cite{KALUZA_2018, bsm}. This is effectively the signature which we can search
for, and we can do so through probing physics at the highest energies accessible
to us.

One new twist that has been added to this idea is that the dynamics are modified
as a parameter of the extra dimension. This term is known as a \textit{warp
factor}. What is compelling about this configuration, is the answer to the
hierarchy problem to modifying the relative strength of the dynamics, depending
on the local position on the manifold. It can be shown that gravity is
\textit{spread out} at the the scale of the standard model. A visual
representation of this is shown in Figure \ref{subfig:warp_factor}. This was
proposed by Lisa Randall and Raman Sundrum, and is called the Randall-Sundrum
geometry \cite{RandallSundrumOriginal, bsm}.

Finally, there is an important detail of this model that is of note to the
experimentalist. That is, the mass towers \textit{are not} evenly spaced
throughout the manifold. The 5-D action potential under Neumann boundary
conditions on the proposed manifold leads to a ``volcano'' like potential for
the resonances. This is shown in Figure \ref{subfig:RSG_resonant_modes}. The
important phenomenological consequence is that the resonant modes are ``pushed''
towards more accessible energies \cite{RandallSundrumOriginal, bsm}. This is why
a direct heavy resonance search of a spin-2 particle is justified - there is
motivated physics behind why such a particle might be observable at TeV scales.

\subsection{Boosted Topology}
Heavy particles will decay to highly energetic states due to mass energy
equivalence. For a di-Higgs state, this means each Higgs will have high
momentum. The highest branching fraction for the Higgs is approximately 57\% to
bottom quarks. Therefore, with the final state of two bottom quarks, the two b
quarks will subsequently also have high momentum. Thus, for a heavy resonant
search, we seek a topology with two bottom quarks kinematically favoring the
direction of the boosted higgs. At a certain energy, the b quark jets are highly
overlapping, and they are better reconstructed as a single large radius jet with
``two-pronged'' topological characteristics. Jets selected for this purpose are
$R=1.0$ Unified Flow Object jets \cite{boosted_hbbcc_tagger, large_r_jet}. A
graphic representation of the boosted b-quark jets is shown in Figure
\ref{subfig:boosted_topology}. 

The primary purpose of this analysis is to perform a direct search for heavy
resonances of a spin-2 Randall-Sundrum graviton. The scattering from gluon-gluon
fusion is shown in Figure \ref{subfig:feynman}. The boosted topology is chosen
due to the sensitivity in the high mass region. We can see the sensitivity of
the \textit{resolved} analysis begins to decrease at masses of approximately
1000-1100 GeV \cite{atlas_resonant_2022}. 


\begin{figure}[t!]
    \centering
    \begin{subfigure}[t]{.48\textwidth}
        \centering
        \includegraphics[width=\columnwidth]{Boosted_Topology}
        \caption{Boosted signal topology showing the two bottom quarks forming as a single large radius jet.}
        \label{subfig:boosted_topology}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{.48\textwidth}
        \centering
        \includegraphics[width=\columnwidth]{G_kk_scattering}
        \caption{Feynman diagram showing the scattering process for a Spin-2
        reonsnance created from gluon-gluon fusion (ggF) decaying to a Higgs pair.}
        \label{subfig:feynman}
    \end{subfigure}
    \caption{Signal topology for the resonant boosted $hh\rightarrow \qqp{b}$.}
\label{fig:signal_topologies}
\end{figure}

\subsection{Xbb Tagging}
For this analysis, it is crucially important to identify $H\rightarrow b\bar{b}
/ c\bar{c}$ processes. For this purpose, we leverage the ATLAS GN2X neural net
for identifying large radius jets in the boosted regime. This model has been
able to improve tagging considerably over the previous approach, which used
flavour tagging discriminants of individual track-based subjets in a basic feed
forward neural net. Contrast this to the GN2X which builds upon previous results
of Graph Neural Net (GNN) networks to tag and calibrate small radius jets, and
adds transformer-encoder architecture.

% In addition there are some variations that have been
% explored combining tracks, subjets, and large R jet calorimetry constituents to
% increase the performance.

GN2X is trained with the goal of classifying large radius jets based on their
origin, that is to discriminate $H\rightarrow b\bar{b}/c\bar{c}$ from background
processes that could produce jets of a similar structure (and substructure).
Backgrounds considered in training were just two - multijet and fully hadronic
top quark decays. Examples of some substructure elements that the tagger would
``learn'' are topological characteristics such as displaced secondary tracks for
b-jets. In addition it may observe the two-pronged nature of the large radius
jet, as it is composed of two smaller sub-jets. Sub-jets within the large radius
jet are defined with a variable radius clustering algorithm
\cite{jet_substructure}. Samples are trained on \textit{associated Z production}
Higgs - $q\bar{q}\rightarrow ZH, Z\rightarrow \mu^+\mu^-$. One difficulty of
this training, is that with a standard Higgs production, we will have a
distribution centered at $125 \text{GeV}$, and any neural network will simply
adjust the weights and biases to select the 125 GeV sample. To overcome this,
the phase space sampling is biased such that the tagger is trained on a flat
mass distribution. This is important for simultaneous jet mass regression and
classification.

% The three main samples used in training are for the signal and two backgrounds.
% For the H jet signal, \textit{associated Z production} higgs samples were
% produced. That is, we have $q\bar{q}\rightarrow ZH, Z\rightarrow \mu^+\mu^-$.
% For top quark jet backgrounds, the process $Z'\rightarrow t\bar{t}$ is used.
% Multijet is simply produced in the standard QCD multijet production within
% ATLAS. One difficulty of this training, is that with a standard Higgs
% production, we will have a distribution centered at $125 \text{GeV}$, and any
% neural network will simply adjust the weights and biases to select the 125 GeV
% sample. To overcome this, the phase space sampling is biased such that the
% tagger is trained on a flat mass distribution. This becomes of utmost importance
% when GN2X is performing simultaneous mass regression for the jet mass
% calibration. However, when it comes to evaluation of the network, a separate
% sample of associated higgs production is used with H set to $125 \text{GeV}$ and
% a smooth, exponentially falling distribution for $p_T$.

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.75\textwidth]{Figure2_GN2X_Tagger_PubNote}
    \caption{Performance of the Xbb tagger.}
    \label{fig:Xbb_tagger}
\end{figure}

The final results are shown in Figure \ref{fig:Xbb_tagger}. Accross the whole
spectrum of tagging efficiencies the GN2X outperforms the previous tagging
algorithm with respect to the number of backgrounds rejected
\cite{boosted_hbbcc_tagger}. This is computed for the two main sources of
background - Top quark and QCD multijet. If the results are normalized to the
previous benchmark - the simple feed-forward DNN, we are able to see the
increased ratio of the background rejection for the new algorithm. From the QCD
multijet backgorund, we see an approximately constant rejection of $2.5$ accross
the spectrum of efficiencies. For Top quark rejection, we see a maximum at
approximately $2.1$ at $.7$ tagging efficiency, with a slow decrease on either
side. We know that QCD is the dominant background, and this background rejection
matters more. Given the increase in overall tagging efficiency, adding the GN2X
to our analysis should in principle yield considerably more sensitive results
than previos analysis.

\subsection{Previous Results}
\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{previous_results_scalar}
        \caption{Previous results with 2018 data for a generic scalar.}
        \label{subfig:previous_results_scalar}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{previous_results_graviton}
        \caption{Previous results with 2018 data for a spin-2 particle.}
        \label{subfig:previous_results_graviton}
    \end{subfigure}
\caption{Past results with resonant analysis for full Run 2 data.}
\label{fig:previous_results}
\end{figure}

Heavy resonance searches have previously been conducted with ATLAS in the Di-Higgs
to four b channel for both boosted and resolved channels. These analysis use
$126 \text{fb}^{-1}$ of data for resolved and $139\text{fb}^{-1}$ for boosted.
This is data from Run 2 operating at $\sqrt{s}=13\text{TeV}$. Both channels
trigger on jets reconstructed with the anti-$k_t$ algorithm \cite{antikt}.
Spin-0 and Spin-2 samples were produced at leading order with MadGraph5
\cite{madgraph5_OG} and simulated with Geant4 \cite{geant4}. Physics objects are
reconstructed with ATLAS Athena software \cite{atlas_simulation}.

This analysis used a different signal, control and validation region than the
the Vector Boson Fusion (VBF) paper \cite{atlas_hhbbbb_vbf}, and current work. The
selections are as follows

\begin{align} \label{eq:vbf_sr}
    SR &= \sqrt{\left( \frac{m_{H1} - 124\un{GeV}}{.1\times m_{H1} } \right)^2 +
    \left( \frac{m_{H2} - 124\un{GeV}}{.1\times m_{H2}} \right)^2} \\
    VR &= \sqrt{\left( m_{H1} - 124\un{GeV} \right)^2 + \left( m_{H2} -
    124\un{GeV} \right)^2} < 33\un{GeV} \\
    VR &= \sqrt{\left( m_{H1} - 124\un{GeV} \right)^2 + \left( m_{H2} -
    124\un{GeV} \right)^2} < 58\un{GeV}
\end{align}

No significant excesses were observed, and there was good agreement between the
background estimation and observed data. Profile likelihood fits were carried
out in bins of $M_{HH}$ for both boosted and resolved channels. Global
significance is computed using the number of crossings below $p=0.5$ is used in
combination with the local $p-\text{value}$ to produce a global
$p-\text{value}$. The most significant excess is found for a signal with mass of
1100 GeV, which had a local significance of $2.3\sigma$. The global significance
was found to be $0.4\sigma$ for the spin-0 resonance and $0.8\sigma$ for spin-2.

Upper limits on produciton cross section for gluon-gluon fusion ($\sigma_{ggF}$)
were produced based on the $CL_S$ method, set at the 95\% confidence level. The
results can be seen in Figure \ref{fig:previous_results}. Below $3\un{TeV}$,
limits are computed with asymptotic formulae. Beyond this mass point, this
approimation is innacurate and thus limits are computed with sampleing
psuedo-experiments. In Figure \ref{fig:previous_results}, a model with a base
case of $k/\overline{M}_{Pl}=1$ is shown. This is an arbitrary number, however
the motivation is such that extra dimensions can explain the \textit{big
hierarchy} problem with little fine tuning, and thus one would want
\textit{natural} values. This is of course a matter of taste. For this
particular choice, masses between the range of $298\;\text{GeV}$ and
$1460\;\text{GeV}$ are excluded \cite{atlas_resonant_2022}. 

\subsection{My Contributions}

Hypothesis testing requires a signal. Therefore, first steps in my contribution
to this analysis were with respect to signal production. Signal production is
generally handled by the Paricle Modelling Group (PMG) within the
collaboration. Old signal production files existed in the archived PMG central
GitLab repository and thus I was able to use a baseline production file. There
were some simple modifications required in order to scale the signal mass width.
Spin-2 graviton signals were produced with MadGraph5, and particle showers
simulated with Pythia 8. The truth Monte-Carlo information is then run through
the full detector simulation with Athena. This procedure was extended to higher
mass ranges than previous samples - 26 all together, from 300 to 6000 GeV. Steps
in signal mass production are gradually increased over the range, which
approximately respects the exponential decrease in background statistics
throughout this range. The full suite of kinematic variables for both particle
flow, and UFO jets were plotted for all mass points. These kinematics histograms
have been inspected for deficiencies and approved by the analysis team.

Monte-Carlo simulation of QCD Multijet backgrounds relies on a specific process
in order to get a physically representative result. It is difficult to produce a
specific number of jets at a particular $p_T$, and therefore it is more
computationally efficient to produce the minimum number events needed and
\textit{reweight} them, in order to get a smoothly falling $p_T$ distribution.
This is done in ten separate ``slices'' of $p_T$. Early studies involved
investigating the kinematic distributions of each $p_T$ slice individually as
well as combined to ensure that we are accurately reconstructing the Monte-Carlo
data.

\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{.48\textwidth}
        \centering
        \includegraphics[width=\columnwidth]{jet_reconstruction}
        \caption{Truth large radius jet mass for the generated signal. Run 3
        data taking conditions are simulated at $\sqrt{s}=13.6\un{TeV}$ with
        Athena \cite{atlas_simulation}. No other selections are applied.}
        \label{subfig:jet_reconstruction}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{.48\textwidth}
        \centering
        \includegraphics[width=\columnwidth]{boosted_m_hh-2023-ROI_incl_SR-2tag_wp85}
        \caption{Signal and background comparison, with the signal region
        selections given in equation \ref{eq:vbf_sr}. Only a subset of the given
        mass range is shown for visual clarity.}
        \label{subfig:input_hists}
    \end{subfigure}
    \caption{Signal topologies, and statistical fitting histogram inputs for the
    Randall-Sundrum graviton. Inputs for the statistical fits are scaled to an
    arbitrary cross section, \textit{i.e.}
    $\sigma_{\text{graviton}}=10\un{ab}^{-1}$.}
\label{fig:my_contributions}
\end{figure}

In addition, other early contributions to the analysis involved comparing the
Monte-Carlo simulated backgrounds with blinded data. This involed studying all
possible permutations of GN2X working points as well as selections, and
investigating if there was closure for the kinematic properties. Moreover, we
wanted to study the overall yields of Monte-Carlo in comparison with the data. Although our
studies demonstrated that \textit{some years of data} had good agreement with
the tighest selections, the general conclusion was that the Monte-Carlo
backgrounds were insufficient for the analysis. The trend appeared to show that
recent data taking years are better with respect to both shape and yields.
However, our results are unsurprising, as it is generally known within ATLAS
that multijet backgrounds are not well modelled due to the intrinsic
complexity of quantum chromodynamic processes.

As we are primarily studying two jets for the boosted analysis, a central tool
for making region of interest selections comes from studying \textit{the mass
plane} formed with the two jets. As our region of interest (ROI) selections are
simply a function of jet mass, producing mass plane plots is important to
visualizing, understanding, and optimizing our signal region definitions. Thus
far I've written analysis code to produce these two dimensional histograms and
superimpose the signal region selections. These ROI selections are still being
studied as to what will provide the optimal sensitivity. The current strategy is
based on the previously mentioned VBF paper \cite{atlas_hhbbbb_vbf}.

\begin{align}
    SR &= \sqrt{\left( \frac{m_{H1}-124\;\text{GeV}}{1500\;\text{GeV}/m_{H1}}
    \right)^2 + \left( \frac{m_{H2}-117\;\text{GeV}}{1900\;\text{GeV}/m_{H2}}
    \right)^2} < 1.6 \un{GeV} \\
    VR &= \sqrt{\left( \frac{m_{H1}-124\;\text{GeV}}{.1\ln(m_{H1}/\;\text{GeV})}
    \right)^2 + \left( \frac{m_{H2}-117\;\text{GeV}}{.1\ln(m_{H2}/\;\text{GeV})}
    \right)^2} < 100 \un{GeV} \\
    CR &= \sqrt{\left( \frac{m_{H1}-124\;\text{GeV}}{.1\ln(m_{H1}/\;\text{GeV})}
    \right)^2 + \left( \frac{m_{H2}-117\;\text{GeV}}{.1\ln(m_{H2}/\;\text{GeV})}
    \right)^2} < 170 \un{GeV}
\end{align}

Crucially, I've written code to pass through the full selection pipeline for all
the various sources of data that we have studied, namely blinded data, signal,
Monte-Carlo background, and the current two candidates for background estimates.
As each data type is loaded, cut selection efficiences are produced and saved.
Finally, a histogram production pipeline has been produced in order to
selectively load given data sources, make the appropriate cuts, and produce
histograms for the signal, data and background in order to pass to the
statistial fitting framework. %This has been done with a command line interface

Finally, as part of the statistical analysis, I have built a pipeline which
takes the histograms and applies the CLS fits to produce exclusion limits.
Unfortunately, the chosen statistical framework (Cabinetry) \cite{cabinetry}
does not handle configurations for multiple fits for many signals. Part of my
recent work has been building and maintaining a wrapper around the statistical
tools such that we can adjust the configuration files on the fly, and run
separate fitting processes for each mass point.

At this point, pushing the resonant interpretation forward is primarily my
responsibility for this analysis.

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=\textwidth]{graviton_overlay}
%     \caption{Spin-2 resonance shown with all mass points. The summation is shown
%     in black. It is clear that there is sufficient overlap between the mass
%     points to cover the intended search range.}
%     \label{fig:graviton_overlay}
% \end{figure}


\subsection{Preliminary Results and Roadmap to Completion}
\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{.48\textwidth}
        \centering
        \includegraphics[width=\columnwidth]{Run2-CLS_fits-m_hh}
        \caption{Fits from Run 2 data.}
        \label{subfig:cls-run2}
    \end{subfigure}
    \begin{subfigure}[t]{.48\textwidth}
        \centering
        \includegraphics[width=\columnwidth]{Run3-CLS_fits-m_hh}
        \caption{Fits from Partial Run 3 data.}
        \label{subfig:cls-run3}
    \end{subfigure}
\caption{Comparison of CLS fits for Run 2 and partial Run 3 data.}
\label{fig:cls_fit_results}
\end{figure}
Thus far we have put together the majority of the pipeline for the analysis.
Preliminary studies have been done of Data/Monte-Carlo comparisons. Signals have been
produced and are ready for hypothesis testing. Early background estimates are
complete. We have studied our Signal, Validation, and Control regions through
the 2-D mass plane and parameterized selections of the ROI. Cuts, data
selections and working points are being investigated as to what may provide the
optimal sensitivity for our statistical fits. Histogram overlays of signal and
backgrounds are produced as inputs for our statistical fitting framework.
Finally, the fitting framework is producing meaningful limits for each mass
point. Ongoing work is being done in order to scale the signal inputs
appropriately for each mass point such that we have a fixed production cross
section in order to have a reference point for our cross section exclusion
plots, produced by the CLS fitting limits. This is shown in Figure
\ref{fig:cls_fit_results}. 

Although there are several more studies needed in order to bring to analysis to
completion, there is a clear roadmap for each. First and foremost, the analysis
is waiting on an updated version of the GN2X tagger. There were some minor
training biases that needed to be updated. There are members of the analysis
team currently working on integrating the updated version into the new data
samples. Background studies are still being iterated on. However, there is a
working baseline background estimate which I contributed to. The remaining tasks
are to understand and improve upon any shape discrepencies, and account for them
with uncertainties. In particular, this is important for the fitting variable
used, that is the di-Higgs system mass $m_{HH}$. At present I am initiating
group discussions with analysis team members to finalize background estimates.
Furthermore, there are some final steps with the statistical fitting methods.
Firstly, I need to properly scale and track the input sigal cross section, in
order to produce exact exclusion limits. Secondly, the fits need to be performed
with combined histograms for Run 2 and Run 3. Finally, once we have our
background estimates thoroughly understood and our full preliminary results
ready, we can propose the paper move forward with the approval process within
ATLAS. This involves getting an editorial board request. Currently, I am in the
process of requesting additional spin-0 samples be produced. Time permitting, we
will be able to add these results to the analysis as well. As discussed earlier
in this article, searches can be performed with both spin-2 and spin-0 samples
with little change to the data pipeline or experimental methods.

The timeline proposed is the following. For the beginning of the 2025 winter
semester, I will have finalized the scaling for the fits, such that we have a
physical exclusion limit. In addition, I will have produced spin-0 samples
locally, and will have circulated kinematic plots for approval within the
analysis team. Upon approval, I can move forward with the signal request from
central production. Throughout the winter semester, the analysis team and I can
finalize the background estimate. During this time I can combine the statistical
analysis for Run 2 and Run 3. That leaves an editorial board request for the end
of the winter 2025 semester, or the first semester of 2026. The iterations with
the editorial board, and the internal ATLAS will take the semester. I propose to
have my thesis ready for application to graduate by the summer semester of 2026.


\section{Conclusion}
Theories of compactified extra dimensions are strongly motivated. They provide a
natural explanation to the hierarchy problem with minimal fine tuning
\cite{RandallSundrumOriginal, bsm}. Furthermore, the geometry may be configured
in such a way that makes resonant signatures available at TeV scales. In
addition, there are many models of physics beyond the standard model that
contain particles which couple to the Higgs. Therefore, the di-Higgs channel is
an interesting channel to perform direct resonant searches on. As the branching
fraction to bottom quarks is the highest, the $\qqp{b}$ channel has the largest
signal, but it also has the highest background. However, the recent improvements
in $\qp{b}/\qp{c}$ tagging will have a strong impact on signal significance.
This analysis builds upon previous ATLAS publication in Higgs pairs
\cite{atlas_resonant_2022, atlas_hhbbbb_vbf}. Therefore, we have a good
understanding of many aspects of the analysis. These include, signal topology,
background estimation techniques, signal, validation and control region
selections, as well as a reference for sensitivity. In addition there have been
improved techniques to large radius jet reconstruction and calibration since the
previous results were published \cite{large_r_jet}. Finally, for this analysis
we have more data at higher center of mass energy. As a result, we expect a
considerable improvement from the previous iteration for signal cross section
exclusion limits, and a much more sensitive overall search. The preliminary
results are promising thus far, and the analysis is proceeding within a
reasonable timeline for graduation.


% For this analysis, we have a very well motivated theoretical model to study. In
% addition, there are many BSM models which couple to the Higgs. Therefore, it is
% strongly motivated to do generic heavy resonance searches, in addition to the
% graviton search. This study can be added with little additional effort, beyond
% the signal request and preliminary data processing, which has negligeable
% processing time. As we are building upon the previous searches done by others
% \cite{atlas_resonant_2022}, we have a good understanding of many aspects of the
% analysis. These include, signal topology, background estimation techniques,
% signal, validation and control region selections, as well as a reference for
% sensitivity. There are several improvements to the general analysis techniques
% that give reason to believe that these results will be more sensitive than
% previous results. First we have a considerably more data. In addition to
% additional integrated luminosity, the $pp$ collisions are occuring at a higher
% center of mass energy. Although this is an incremental increase, it will have a
% disproportionate effect on heavy resonance searches. Furthermore, the
% improvements to Xbb tagging (a roughly $2.5\times$ improvement), will yield a
% higher signal sensitivity than before. In addition, there have been improved
% techniques to large radius jet reconstruction and calibration since the previous
% results were published \cite{large_r_jet}. The preliminary results are promising
% thus far, and the analysis is proceeding within a reasonable timeline for
% graduation.

%% BIBLIOGRAPHY
\newpage
\bibliographystyle{unsrt}
\bibliography{bibliography}

\end{document}

