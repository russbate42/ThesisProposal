\documentclass[12pt]{article}

\usepackage{import} % for importing alternative to include\input
\input{preamble}
\input{geometry}
\input{commands}
\graphicspath{{figures/}}
% \usepackage{glossaries}
% \input{header}

\usepackage{silence}
\WarningsOff[lipsum]
\WarningFilter[lipsum]{lipsum}{Package lipsum Warning}
\usepackage{lipsum}
\import{./}{dummy-text-generator.tex}

% Math command
\newcommand{\pd}{\partial}

% For glossary without glossaries package
\newcommand{\term}[2]{\textbf{#1}: #2}
\newcommand{\acronym}[2]{\textbf{#1} -- #2}

\begin{document}
\ActivateWarningFilters[lipsum]
\import{./}{title-page.tex}

\newpage
\section{Introduction}
This article is a thesis proposal for a Doctorate of Philosophy from the Physics
and Astronomy Department at the University of British Columbia. The research
presented here uses data from the Large Hadron Collider (LHC) at the European
Organization for Nuclear Research (CERN). This proposal covers key topics
related to my thesis. Firstly, a terse outline of the ATLAS detector will be
given as it pertains to the recorded data used in the research discussed here.
Secondly I will outline my work with the organization and other research
projects I have been involved with thus far. Next it will cover the topics
pertaining to my thesis, namely theoretical motivation, a brief outline of
previous research on this topic, as well as new techniques and methods being
applied. Finally, we will look at progress thus far in the new studies as well
as preliminary results.

\section{The ATLAS Experiment}
The ATLAS experiment is a general purpose particle detector. It stands for
\textbf{A} \textbf{T}oroidal \textbf{L}HC \textbf{A}pparatu\textbf{S} getting
it's name from the toroidal magnet system which is integral to the design which
plays an important role in physics observations. ATLAS is designed to observe
collisions from the full LHC beam energy and intensity. The overall dimensions
are 44 meters across, with a diameter of 25 meters, and weighs over 7000 tonnes
\cite{Aad_2024}. \begin{wrapfigure}{l}{0.42\textwidth}
    \centering
    \includegraphics[width=0.4\textwidth]{Aerial_view_of_the_area_where_the_LHC_ring_is_located}
    \caption{Aerial view of the LHC}
    \label{fig:aerial_view}
\end{wrapfigure} As a general purpose particle detector it is a composite of
many different subdetector systems, each with their own physics objectives in
mind. The detector covers nearly the entire solid angle around the
collision point \cite{The_ATLAS_Collaboration_2008}. 

The LHC beam can supply protons and heavy ion nuclei at energies up to $13.6$
TeV \cite{Aad_2024}. Particles are injected into the LHC
ring in bunches from the SPS beam. Bunches are held in place with Radio
Frequency (RF) electric fields, with a given spacing. During comissioning and
testing, bunches can be partially filled, and the LHC has a specified
\textit{bunch fill pattern} \cite{bunch_filling_schemes_bailey}. During data
taking however, the LHC is nominally operating with a full bunch fill. During
beam preparations, when the fill is complete, the beam energy is slowly ramped
up to target operational energies. Bunches are guided around the LHC ring with
dipole magnets \cite{The_ATLAS_Collaboration_2008,
bunch_filling_schemes_bailey}. At operational energies, particles are highly
Lorentz boosted, and have a bunch crossing spacing of $25$ ns. Collisions occur
at the crossing point located at the center of the detector, and are subject to
\textit{beam optics}. Beam optics govern the number of collisions per bunch
crossing through the density of bunches and accuracy of the collision point.
Higher beam collimation leads to a higher collision rate, which is known as
\textit{instantaneous luminosity}. Beam collimation, and collision geometries
are grouped into a single parameter at the LHC - $\beta^*$ \cite{Aad_2024}. The
many collisions that occur during a single bunch crossing is known as \textit{in
time pile-up} and is paramaterized by $\langle\mu\rangle$. This depends on beam
conditions during each data taking year.

The fundamental equation for collider physics in differential form is
$\mathcal{N}=\sigma\cdot\mathscr{L}$, where $\mathcal{N}$ is the rate of
collisions, $\sigma$ is the scattering cross section, and $\mathscr{L}$ is the
\textit{instantaneous luminosity}. As data recording conditions change, this
equation is more commonly represented in it's time integrated form, over the
operational lifetime of interest. Therefore, we have $N=\sigma\cdot L$ where $N$
is the total number of events, and $L$ is the \textit{integrated luminosity}.
The total integrated luminosity used in this analysis is shown in Figure
\ref{fig:lhc_luminosity}, under the conditions outlined in reference
\cite{Aad_2024}. 

\begin{figure}[t!]
    \centering
    \begin{subfigure}[t]{.48\textwidth}
        \centering
        \includegraphics[width=\columnwidth]{intlumivstimeRun2}
        \caption{Luminosity collected from the full LHC Run 2.}
        \label{subfig:lumi_run2}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{.48\textwidth}
        \centering
        \includegraphics[width=\columnwidth]{intlumivstimeRun3}
        \caption{Luminosity collected from the partial LHC Run 3.}
        \label{subfig:lumi_run3}
    \end{subfigure}
\caption{Luminosity delivered to ATLAS from the LHC for the duration of data
taking for the analysis - that is, Full Run 2 data at $\sqrt{s}=13\un{TeV}$, and
partial Run 3 at $\sqrt{s}=13.6\un{TeV}$.} %(cite me)
\label{fig:lhc_luminosity}
\end{figure}

\section{Research Contributions to ATLAS}

% \section{Machine Learning for Pions}
% \begin{figure}[h]
%     \centering
%     \begin{subfigure}[t]{.48\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{EnergyIQROverMedian}
%         \caption{Neural network response demonstrating energy reconstruction
%         median and the interquartile range.}
%         \label{sufig:ml4p_energy_response}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[t]{.48\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{clusters_tracks_combined}
%         \caption{Figure demonstrating the energy response complementa}
%         \label{subfig:complimentarity}
%     \end{subfigure}
% \caption{Main results from the machine learning for pions project.}
% \label{fig:ml4p_main_results.}
% \end{figure}


\subsection{Machine Learning for Pions}
My first research project within ATLAS as with the Machine Learning for Pions
project. At ATLAS, an inportant step in physics analysis is the reconstruction
of incident particle energy. One decay mechanism for a particle with some high
initial energy is to form energetic particle showers within the detector. Thus
with the primary goal in mind of reconstructing fundamental interactions, we
seek to accurately reconstruct the total energy of particle showers, henceforth
referred to as \textit{jets}. The Machine Learning for Pions project had a goal
of investigating if machine learning techniques could improve not only energy
reconstruction but also incident particle classification as well
\cite{ml4p_prelim}. This was done with Pions due the fact that as a light
hadron, they are produced in abundance at the LHC and are a primary
reconstruction object within a jet. This research project was largely a success,
demonstrating that deep learning was able to better reconstruct energies of jet
substructure elements when compared with the standard technique used within
ATLAS of Local Cell Weighting (LCW) \cite{ml4p}. Moreoever, we were able to
demonstrate the use of complimentary information from the detector performed
better than using this information on an individual basis \cite{ml4p}.
% See Appendix \ref{subfig:complimentarity} for the relevant plot, demonstrating
% subdetector complementarity.
% Add citation for LCW

\subsection{Time at CERN}
During my PhD I was given the opportunity to travel to the LHC and work on site
at CERN. This was a valuable opportunity to collaborate in person with
researchers from my team on various projects. In addition, I was able to take
part in underground visits to see the ATLAS detector in person. Beyond seeing
the detector in person, I was able to get trained and take shifts in the ATLAS
control room. My responsibilities involved two positions, the Run Controller,
and the Trigger Shifter. The Run controller is responsible for starting and
stopping data daking runs, and choosing which subcomponents of the detector are
involved in recording data. The trigger shifter is responsible for monitoring
data rates, CPU processing usage, and controlling which events we will record,
as dictated by the \textit{trigger menu} \cite{trigger_2017, HLT_2016}. As a
result, I actively took part in data taking for some of the data used in this
analysis, as shown in Figure \ref{fig:lhc_luminosity}.



\subsection{Authorship Qualification Project}
Active collaborators with the ATLAS experiment must complete some task that is
needed by the experimental apparatus (or software) in order to qualify for
authorship on papers. For this task, I worked on studies related to an important
step in data processing called \textit{jet cleaning}. When we take data with
ATLAS, not all of the jets which we record are from the physics events we are
interested in. Jets which are reconstructed which are \textit{not} from
proton-proton collisions are referred to as \textit{fake jets}. There are three
main sources of fake jets. These are calorimetry noise, beam induced background
(BIB) or events from cosmic muons causing showers in the detector. In order to
remove these events, we apply jet cleaning cuts to the data. The strength of
cuts are fixed at given \textit{working points}. It was my task to investigate
if these cuts were working as expected for the newest internal software releases
\cite{atlas_simulation}, and characterize their performance, as well as perform
studies on how we might possibly optimize some of these cuts.

Within ATLAS, we do not simulate fake jet events, and therefore we
have no truth information to characterize these types of events or perhaps
implement a supervised learning algorithm. Therefore, cuts are developed based
on intuitive physics. For example, examining the energy distribution of an
event. If energy is deposited horizontally within the detector, there is a
higher likelihood that it came from beam induced background which are
approximately parallel with the beam axis. Early cleaning cuts were developed
based on known hardware issues such as calorimetry noise. Due to the types of
selections that are being made and the instrinsic difficulty of the problem,
there is considerable room for optimization. 

\begin{figure}[t!]
    \centering
    \begin{subfigure}[t]{.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{GoodJets_FracSamplingMax_EMFrac}
        \caption{Balanced di-jet sample showing physics of well behaved jets.}
        \label{subfig:good_jets_th2}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{FakeJets_FracSamplingMax_EMFrac}
        \caption{Fake jet enriched sample demonstrating a strong bias towards
        distribution tails.}
        \label{subfig:fake_jets_th2}
    \end{subfigure} %
    ~
    \begin{subfigure}[t]{.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Trees_NewDef_R22_Eta_Efficiency_Tight}
        \caption{Jet cleaning efficiency for the tight working point,
        characterized accross $|\eta|$.}
        \label{subfig:cleaning_perf_th2_loose}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Trees_NewDef_R22_FakeRejection_Tight}
        \caption{Fake jet rejection for the tight working point as a function of
        $p_T$.}
        \label{subfig:cleaning_perf_th2_tight}
    \end{subfigure}
\caption{Jet cleaning studies, showing kinematic distributions for balanced
di-jets and fakd jet enriched samples, as well as the efficiency and rejection
rates for the tight working point.}
\label{fig:jet_cleaning}
\end{figure}
Optimization studies were performed by searching for ``bumps'' in
distribution tails. In order to aid these studies, two important subsamples of
collision data were created. Firstly, we look at kinematically balanced di-jet
samples. There is no guarantee that all events in this sample are from $pp$
scattering events, however due to the nature of the selections we can assume a
high purity. The second category is one which has a higher likelihood of
contamination from fake jet events. For this, selections are used such as jets
being \textit{out of time}, or events which have high missing transverse
momentum ($p_T$). These two samples are compared in both one and two dimensions with
variables that we believe to be of interest for the cleaning cuts. It is clear
that the enriched fake jet sample has properties in the tails of the
distributions that are indicative of sample contamination. See Figure
\ref{subfig:fake_jets_th2}. 

The performance of these cuts can be characterized with respect to two metrics.
First, we measure the cut efficiency using the balanced di-jet sample. This is
done with the \textit{tag and probe} method. Simply put, tag and probe relies on
the assumption that in our balanced di-jet sample, if the leading $p_T$ jet is a
fake, so too is it's counterpart. Applying cleaning cuts to the balanced di-jet
sample on \textit{both} leading \textit{and} subleading jets, denominated by
cuts applied to \textit{only} leading \textit{or} subleading, yields an
approximation for efficiency. Secondly we measure the number of jets rejected
from the fakes, keeping in mind that the degree of purity is unkown, and
therefore rejecting all fakes is not necessarily better. These studies are
performed with small radius Particle Flow jets \cite{pflow_jets}. 

Three working points for these cuts are chosen - loose, tight, and tenacious -
each with successively increasing fake jet rejection, and decreasing efficiency.
The performance of these cuts has been characterized across detector geometries
and transverse momentum distributions for each working point. This is performed
for both Run 2 and Run 3 data. These working points were integrated into the
newest ATLAS Athena software \cite{atlas_simulation} releases.

\section{Resonant Analysis of Boosted Di-Higgs to Four Bottom Quarks
$hh\rightarrow b\overline{b}b\overline{b}$}
\subsection{Theoretical Motivation}

The Higgs boson is an interesting particle to perform direct searches with
because there is such a rich ecosystem of physics beyond the standard model that
can \textit{and should} in principle couple to the higgs. In general, unless
there is a specific search that is extremely well motivated, it is a worthwhile
endeavour to do the broadest search possible in order to capture as much of this
ecosystem at once. Therefore, it is in the experimentalists best interest to
keep any direct search as model agnostic as possible. With that in mind, there
is such an idea that has not yet been ruled out experimentally and is very well
motivated by one of the largest questions in theoretical physics. That is,
\textit{big hierarchy problem}. Namely, \textit{why is gravity so weak in
comparison with other forces?}. The theoretical construct in question is that of
Extra Dimensions. This idea is not new, and in fact has been around for a long
time, since the 1920s \cite{KALUZA_2018}. A phenomelogical consequence of extra
dimensions is a spin-2 graviton which under no other assumptions can couple to
the Higgs boson \cite{bsm}.  

The obvious extension is that of flat extra dimensions. It is easy to extend
flat Minkowski space by describing the metric as $\eta = (-1,1,1,1,1)$. However,
one of the consequences of having large flat extra dimensions is that we may
expect some deviations from the inverse square law of gravity \cite{bsm}. Some
experiments have been devised to measure this deviation at small scale, and a
large amount of phase space has been ruled out \cite{bsm}. However, this is
dependent upon the number of extra dimensions. If we add extra dimensions of
$n\geq 2$, it is still possible to provide some natural explanation for the
hierarchy problem. Gravity is weak in comparison to Electro-Weak (EW) forces,
because EW is confined to the three spatial dimensions, and the graviton is able
to propagate in some higher dimensional manifold. These are known as as Large
Extra Dimension (LED), or Arkani-Hamed, Dimopoulos, Dvali (ADD) models
\cite{Arkani_Hamed_1998, Arkani_Hamed_1999}. 

\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{RSG_resonant_modes.png}
        \caption{Visual representation of the resonant modes resulting from the
        solution of the equation in warped extra dimensions. The resonant modes
        are pushed out to the boundary which is the energy limit for the physics we
        wish to study \cite{bsm}.}
        \label{subfig:RSG_resonant_modes}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{warp_factor}
        \caption{Graphic representation of the relative strength of gravity
        spreading out at the boundary \cite{bsm}.}
        \label{subfig:warp_factor}
    \end{subfigure}
\caption{}
\label{fig:theoretical_motivation_figure}
\end{figure}

It is not difficult to imagine a slightly more complex geometry where instead of
additional flat dimensions we can add some curvature. One way to achieve this is
to simply form a closed loop with one of the dimensions. This process is
generally referred to as \textit{compactification}. In the most simple case of a
compactified dimension, the closed loop creates a closed boundary condition. As
a result, resonance can occur with higher order excitations of fields on this
manifold. 

% Maybe swap these two paragraphs
First let us make some fundamental observations about allowing for extra
dimensions. Momentum in a five (or higher) dimensional spacetime can show up as
extra mass in four-space \cite{KALUZA_2018, bsm}. 
\begin{align}
    E = \sqrt{\vb{p}^2 + p_5^5 + m^2} \\
    E = \sqrt{\vb{p}^2 + m_{KK}}
\end{align}
Crucially, this is only the case for particles which have access to the 5-D
bulk. Standard model particles are restricted to three spatial dimensions.
Solving for the 5-D action potenial of a scalar particle on the simple
compactified 5-D manifold configuration, and allowing the wavefunction to be
periodic, allows for distinct resonant modes to form. Namely, we have
\begin{equation}
    m^2 = m_0^2 + \frac{n_5^2}{R_5^2}
\end{equation}
Where $R$ is the radius of the compactified dimension, and we have the $n^{th}$
order field excitation. These are what are known as Kaluza-Klein mass towers
\cite{KALUZA_2018, bsm}. This is effectively the signature which we can search
for, and we can do so through probing physics at the highest energies accessible
to us.

One new twist that has been added to this idea is that the dynamics are modified
as a parameter of the extra dimension. This term is known as a \textit{warp
factor}. What is compelling about this configuration, is the answer to the
hierarchy problem to modifying the relative strength of the dynamics, depending
on the local position on the manifold. It can be shown that gravity is
\textit{spread out} at the the scale of the standard model. A visual
representation of this is shown in Figure \ref{subfig:warp_factor}. This was
proposed by Lisa Randall and Raman Sundrum, and is called the Randall-Sundrum
geometry \cite{RandallSundrumOriginal, bsm}.

Finally, there is an important detail of this model that is of note to the
experimentalist. That is, the mass towers \textit{are not} evenly spaced
throughout the manifold. The 5-D action potential under Neumann boundary
conditions on the proposed manifold leads to a ``volcano'' like potential for
the resonances. This is shown in Figure \ref{subfig:RSG_resonant_modes}. The
important phenomenological consequence is that the resonant modes are ``pushed''
towards more accessible energies \cite{RandallSundrumOriginal, bsm}. This is why
a direct heavy resonance search of a spin-2 particle is justified.


\subsection{Boosted Topology}
Heavy particles will decay to highly energetic states due to mass energy
equivalence. For a di-Higgs state, this means each Higgs will have high
momentum. The highest branching fraction for the Higgs is approximately 57\% to
bottom quarks. Therefore, with the final state of two bottom quarks, the two b
quarks will subsequently also have high momentum. Thus, for a heavy resonant
search, we seek a topology with two bottom quarks kinematically favoring the
direction of the boosted higgs. At a certain energy, the b quark jets are highly
overlapping, and they are better reconstructed as a single large radius jet with
``two-pronged'' topological characteristics. Jets selected for this purpose are
$R=1.0$ Unified Flow Object jets \cite{boosted_hbbcc_tagger, large_r_jet}. A
graphic representation of the boosted b-quark jets is shown in Figure
\ref{subfig:boosted_topology}. 

The primary purpose of this analysis is to perform a direct search for heavy
resonances of a spin-2 Randall-Sundrum graviton. The scattering from gluon-gluon
fusion is shown in Figure \ref{subfig:feynman}. The boosted topology is chosen
due to the sensitivity in the high mass region. We can see the sensitivity of
the \textit{resolved} analysis begins to decrease at masses of approximately
1000-1100 GeV \cite{atlas_resonant_2022}. 

\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{.48\textwidth}
        \centering
        \includegraphics[width=\columnwidth]{Boosted_Topology}
        \caption{Boosted signal topology showing the two bottom quarks forming as a single large radius jet.}
        \label{subfig:boosted_topology}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{.48\textwidth}
        \centering
        \includegraphics[width=\columnwidth]{G_kk_scattering}
        \caption{Feynman diagram showing the scattering process for a Spin-2
        reonsnance created from gluon-gluon fusion (ggF) decaying to a Higgs pair.}
        \label{subfig:feynman}
    \end{subfigure}
    \caption{Signal topology for the resonant boosted $hh\rightarrow \qqp{b}$.}
\label{fig:signal_topologies}
\end{figure}


\subsection{Xbb Tagging}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.75\textwidth]{Figure2_GN2X_Tagger_PubNote}
    \caption{Performance of the Xbb tagger.}
    \label{fig:Xbb_tagger}
\end{figure}

For many physics objectives within ATLAS, it is important to identify
$H\rightarrow b\bar{b} / c\bar{c}$ processes. ATLAS has implemented the GN2X tagger
for identifying jets originating from such events. In the boosted regime, we use
the GN2X tagger for identifying large radius jets. The GN2X has been able to
improve tagging considerably over the previous approach, which used flavour
tagging discriminants of individual track-based subjets in a basic feed forward
neural net. Contrast this to the GN2X which builds upon previous results of
Graph Neural Net (GNN) networks to tag and calibrate small radius jets, and adds
trgoemetryansformer architecture. In addition there are some variations that have been
explored combining tracks, subjets, and large R jet calorimetry constituents to
increase the performance.

GN2X is trained with the goal of classifying large radius jets based on their
origin, that is to discriminate $H\rightarrow b\bar{b}/c\bar{c}$ from background
processes that could produce jets of a similar structure (and substructure).
Backgrounds considered in training were just two - multijet and fully hadronic
top quark decays. Examples of some substructure elements that the tagger would
``learn'' are topological characteristics such as displaced secondary tracks for
b-jets. In addition it may observe the two-pronged nature of the large radius
jet, as it is composed of two smaller sub-jets. Subjects within the large radius
jet are defined with a variable radius clustering algorithm
\cite{jet_substructure}.

The three main samples used in training are for the signal and two backgrounds.
For the H jet signal, \textit{associated Z production} higgs samples were
produced. That is, we have $q\bar{q}\rightarrow ZH, Z\rightarrow \mu^+\mu^-$.
For top quark jet backgrounds, the process $Z'\rightarrow t\bar{t}$ is used.
Multijet is simply produced in the standard QCD multijet production within
ATLAS. One difficulty of this training, is that with a standard Higgs
production, we will have a distribution centered at $125 \text{GeV}$, and any
neural network will simply adjust the weights and biases to select the 125 GeV
sample. To overcome this, the phase space sampling is biased such that the
tagger is trained on a flat mass distribution. This becomes of utmost importance
when GN2X is performing simultaneous mass regression for the jet mass
calibration. However, when it comes to evaluation of the network, a separate
sample of associated higgs production is used with H set to $125 \text{GeV}$ and
a smooth, exponentially falling distribution for $p_T$.

The final results are shown in Figure \ref{fig:Xbb_tagger}. Accross the whole
spectrum of tagging efficiencies the GN2X outperforms the previous tagging
algorithm with respect to the number of backgrounds rejected
\cite{boosted_hbbcc_tagger}. This is computed for the two main sources of
background - Top quark and QCD multijet. If the results are normalized to the
previous benchmark - the simple feed-forward DNN, we are able to see the
increased ratio of the background rejection for the new algorithm. From the QCD
multijet backgorund, we see an approximately constant rejection of $2.5$ accross
the spectrum of efficiencies. For Top quark rejection, we see a maximum at
approximately $2.1$ at $.7$ tagging efficiency, with a slow decrease on either
side. We know that QCD is the dominant background, and this background rejection
matters more. Given the increase in overall tagging efficiency, adding the GN2X
to our analysis should yield more sensitive results than previos analysis.

\subsection{Previous Results}
\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{previous_results_scalar}
        \caption{Previous results with 2018 data for a generic scalar.}
        \label{subfig:previous_results_scalar}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{previous_results_graviton}
        \caption{Previous results with 2018 data for a spin-2 particle.}
        \label{subfig:previous_results_graviton}
    \end{subfigure}
\caption{Past results with resonant analysis for full Run 2 data.}
\label{fig:previous_results}
\end{figure}

Heavy resonance searches have previously been conducted with ATLAS in the Di-Higgs
to four b channel for both boosted and resolved channels. These analysis use
$126 \text{fb}^{-1}$ of data for resolved and $139\text{fb}^{-1}$ for boosted.
This is data from Run 2 operating at $\sqrt{s}=13\text{TeV}$. Both channels
trigger on jets reconstructed with the anti-$k_t$ algorithm \cite{antikt}.
Spin-0 and Spin-2 samples were produced at leading order with MadGraph5
\cite{madgraph5_OG} and simulated with Geant4 \cite{geant4}. Physics objects are
reconstructed with ATLAS Athena software \cite{atlas_simulation}.

This analysis used a different signal, control and validation region than the
the Vector Boson Fusion (VBF) paper \cite{atlas_hhbbbb_vbf}, and current work. The
selections are as follows

\begin{align}
    SR &= \sqrt{\left( \frac{m_{H1} - 124\un{GeV}}{.1\times m_{H1} } \right)^2 +
    \left( \frac{m_{H2} - 124\un{GeV}}{.1\times m_{H2}} \right)^2} \\
    VR &= \sqrt{\left( m_{H1} - 124\un{GeV} \right)^2 + \left( m_{H2} -
    124\un{GeV} \right)^2} < 33\un{GeV} \\
    VR &= \sqrt{\left( m_{H1} - 124\un{GeV} \right)^2 + \left( m_{H2} -
    124\un{GeV} \right)^2} < 58\un{GeV}
\end{align}

No significant excesses were observed, and there was good agreement between the
background estimation and observed data. Profile likelihood fits were carried
out in bins of $M_{HH}$ for both boosted and resolved channels. Global
significance is computed using the number of crossings below $p=0.5$ is used in
combination with the local $p-\text{value}$ to produce a global
$p-\text{value}$. The most significant excess is found for a signal with mass of
1100 GeV, which had a local significance of $2.3\sigma$. The global significance
was found to be $0.4\sigma$ for the spin-0 resonance and $0.8\sigma$ for spin-2.

Upper limits on produciton cross section for gluon-gluon fusion ($\sigma_{ggF}$)
were produced based on the $CL_S$ method, set at the 95\% confidence level. The
results can be seen in Figure \ref{fig:previous_results}. Below $3\un{TeV}$,
limits are computed with asymptotic formulae. Beyond this mass point, this
approimation is innacurate and thus limits are computed with sampleing
psuedo-experiments. In Figure \ref{fig:previous_results}, a model with a base
case of $k/\overline{M}_{Pl}=1$ is shown. This is an arbitrary number, however
the motivation is such that extra dimensions can explain the \textit{big
hierarchy} problem with little fine tuning, and thus one would want
\textit{natural} values. This is of course a matter of taste. For this choice,
masses between the range of $298\;\text{GeV}$ and $1460\;\text{GeV}$ are
excluded \cite{atlas_resonant_2022}. 

\subsection{My Contributions}

\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{.48\textwidth}
        \centering
        \includegraphics[width=\columnwidth]{jet_reconstruction}
        \caption{Truth jet mass for large radius jets, reconstructed from
        simulated Run 3 events at $\sqrt{s}=13.6\un{TeV}$.}
        \label{subfig:jet_reconstruction}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{.48\textwidth}
        \centering
        \includegraphics[width=\columnwidth]{boosted_m_hh-2023-ROI_incl_SR-2tag_wp85}
        \caption{Input histograms for the statistical fitting, showing only a
        select group of mass points for visual purposes.}
        \label{subfig:input_hists}
    \end{subfigure}
    \caption{Randall-Sundrum graviton signal.}
\label{fig:my_contributions}
\end{figure}
Hypothesis testing requires a signal. Therefore, first steps in my contribution
to this analysis were with respect to signal production. Signal production is
generally handled by the Paricle Modelling Group (PMG) within the
collaboration. Old signal production files existed in the archived PMG central
GitLab repository and thus I was able to use a baseline production file. There
were some simple modifications required in order to scale the signal mass width.
Spin-2 graviton signals were produced with MadGraph5, and particle showers
simulated with Pythia 8. The truth Monte-Carlo information is then run through
the full detector simulation with Athena. This procedure was extended to higher
mass ranges than previous samples - 26 all together, from 300 to 6000 GeV. Steps
in signal mass production are gradually increased over the range, which
approximately respects the exponential decrease in background statistics
throughout this range. The full suite of kinematic variables for both particle
flow, and UFO jets were plotted for all mass points. These kinematics histograms
have been inspected for deficiencies and approved by the analysis team.

Monte-Carlo simulation of QCD Multijet backgrounds relies on a specific process
in order to get a physically representative result. It is difficult to produce a
specific number of jets at a particular $p_T$, and therefore it is more
computationally efficient to produce the minimum number events needed and
\textit{reweight} them, in order to get a smoothly falling $p_T$ distribution.
This is done in ten separate ``slices'' of $p_T$. Early studies involved
investigating the kinematic distributions of each $p_T$ slice individually as
well as combined to ensure that we are accurately reconstructing the Monte-Carlo
data.

In addition, other early contributions to the analysis involved comparing the
Monte-Carlo simulated backgrounds with blinded data. This involed studying all
possible permutations of GN2X working points as well as selections, and
investigating if there was closure for the kinematic properties. Moreover, we
wanted to study the overall yields of Monte-Carlo in comparison with the data. Although our
studies demonstrated that \textit{some years of data} had good agreement with
the tighest selections, the general conclusion was that the Monte-Carlo
backgrounds were insufficient for the analysis. The trend appeared to show that
recent data taking years are better with respect to both shape and yields.
However, our results are unsurprising, as it is generally known within ATLAS
that multijet backgrounds are not well modelled due to the intrinsic
complexity of quantum chromodynamic processes.

As we are primarily studying two jets for the boosted analysis, a central tool
for making region of interest selections comes from studying \textit{the mass
plane} formed with the two jets. As our ROI selections are simply a function of
jet mass, producing mass plane plots is important to visualizing, understanding,
and optimizing our signal region definitions. Thus far I've written analysis
code to produce these two dimensional histograms and superimpose the signal
region selections. These roi selections are still being studied as to what will
provide the optimal sensitivity. The current strategy for ROI is based on the
Vector Boson Fusion (VBF) paper \cite{atlas_hhbbbb_vbf}.

\begin{align}
    SR &= \sqrt{\left( \frac{m_{H1}-124\;\text{GeV}}{1500\;\text{GeV}/m_{H1}}
    \right)^2 + \left( \frac{m_{H2}-117\;\text{GeV}}{1900\;\text{GeV}/m_{H2}}
    \right)^2} < 1.6 \un{GeV} \\
    VR &= \sqrt{\left( \frac{m_{H1}-124\;\text{GeV}}{.1\ln(m_{H1}/\;\text{GeV})}
    \right)^2 + \left( \frac{m_{H2}-117\;\text{GeV}}{.1\ln(m_{H2}/\;\text{GeV})}
    \right)^2} < 100 \un{GeV} \\
    CR &= \sqrt{\left( \frac{m_{H1}-124\;\text{GeV}}{.1\ln(m_{H1}/\;\text{GeV})}
    \right)^2 + \left( \frac{m_{H2}-117\;\text{GeV}}{.1\ln(m_{H2}/\;\text{GeV})}
    \right)^2} < 170 \un{GeV}
\end{align}

Crucially, I've written code to pass through the full selection pipeline for all
the various sources of data that we have studied, namely blinded data, signal,
Monte-Carlo background, and the current two candidates for background estimates.
As each data type is loaded, cut selection efficiences are produced and saved.
Finally, a histogram production pipeline has been produced in order to
selectively load given data sources, make the appropriate cuts, and produce
histograms for the signal, data and background in order to pass to the
statistial fitting framework. %This has been done with a command line interface

Finally, as part of the statistical analysis, I have built a pipeline which
takes the histograms and applies the CLS fits to produce exclusion limits.
Unfortunately, the chosen statistical framework (Cabinetry) \cite{cabinetry}
does not handle configurations for multiple fits for many signals. Part of my
recent work has been building and maintaining a wrapper around the statistical
tools such that we can adjust the configuration files on the fly, and run
separate fitting processes for each mass point.

At this point, pushing the resonant interpretation forward is primarily my
responsibility for this analysis.

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=\textwidth]{graviton_overlay}
%     \caption{Spin-2 resonance shown with all mass points. The summation is shown
%     in black. It is clear that there is sufficient overlap between the mass
%     points to cover the intended search range.}
%     \label{fig:graviton_overlay}
% \end{figure}


\subsection{Preliminary Results and Ongoing Studies}
\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{.48\textwidth}
        \centering
        \includegraphics[width=\columnwidth]{Run2-CLS_fits-m_hh}
        \caption{Fits from Run 2 data.}
        \label{subfig:cls-run2}
    \end{subfigure}
    \begin{subfigure}[t]{.48\textwidth}
        \centering
        \includegraphics[width=\columnwidth]{Run3-CLS_fits-m_hh}
        \caption{Fits from Partial Run 3 data.}
        \label{subfig:cls-run3}
    \end{subfigure}
\caption{Comparison of CLS fits for Run 2 and partial Run 3 data.}
\label{fig:cls_fit_results}
\end{figure}
Thus far we have put together the majority of the pipeline for the analysis.
Preliminary studies have been done of Data/Monte-Carlo comparisons. Signals have been
produced and are ready for hypothesis testing. Early background estimates are
complete. We have studied our Signal, Validation, and Control regions through
the 2-D mass plane and parameterized selections of the ROI. Cuts, data
selections and working points are being investigated as to what may provide the
optimal sensitivity for our statistical fits. Histogram overlays of signal and
backgrounds are produced as inputs for our statistical fitting framework.
Finally, the fitting framework is producing meaningful limits for each mass
point. Ongoing work is being done in order to scale the signal inputs
appropriately for each mass point such that we have a fixed production cross
section in order to have a reference point for our cross section exclusion
plots, produced by the CLS fitting limits. This is shown in Figure
\ref{fig:cls_fit_results}. 

\section{Conclusion}
For this analysis, we have a very well motivated theoretical model to study. In
addition, there are many BSM models which couple to the Higgs. Therefore, it is
strongly motivated to do generic heavy resonance searches, in addition to the
graviton search. This study can be added with little additional effort, beyond
the signal request and preliminary data processing, which has negligeable
processing time. As we are building upon the previous searches done by others
\cite{atlas_resonant_2022}, we have a good understanding of many aspects of the
analysis. These include, signal topology, background estimation techniques,
signal, validation and control region selections, as well as a reference for
sensitivity. There are several improvements to the general analysis techniques
that give reason to believe that these results will be more sensitive than
previous results. First we have a considerably more data. In addition to
additional integrated luminosity, the $pp$ collisions are occuring at a higher
center of mass energy. Although this is an incremental increase, it will have a
disproportionate effect on heavy resonance searches. Furthermore, the
improvements to Xbb tagging (a roughly $2.5\times$ improvement), will yield a
higher signal sensitivity than before. In addition, there have been improved
techniques to large radius jet reconstruction and calibration since the previous
results were published \cite{large_r_jet}. The preliminary results are promising
thus far, and the analysis is proceeding within a reasonable timeline for
graduation.

%% BIBLIOGRAPHY
\newpage
\bibliographystyle{unsrt}
\bibliography{bibliography}

\end{document}

